## 研究背景

在计算机视觉领域，研究的主体对象可以分为图片和视频。视频分析是对视频的内容进行理解进而通过各种方法实现高级语义的分析。其应用场景包括安防、自动驾驶、新零售、视频营销、视频编辑、视频检索等，是目前计算机视觉领域前沿新颖的研究方向。

关于视频分析，主要的研究问题有：
- Temporal Action Detection, Video Classification, Video Tracking, Video Captioning, Video Understanding

这里详细介绍一下Temporal Action Detection(Temporal Action Localizition)。

## 任务目的

Temporal Action Detection又名时序动作检测。

在给定一段未分割的长视频，算法需要检测视频中的行为片段(action instance)，包括其开始时间(starting time)、结束时间(ending time)以及类别(class)。一段视频中可能包含一个或多个行为片段。

评测指标

常用的测评指标为AR (Average recall)、mAP (mean Average Precision)。

AR：Temporal Action Proposal任务不需要对活动分类，只需要找出proposals，所以判断找的temporal proposals全不全就可以测评方法好坏，常用average recall (AR) ，Average Recall vs. Average Number of Proposals per Video (AR-AN) 即曲线下的面积(ActivityNet Challenge 2017就用这个测评此项任务)。如下图：

mAP：Temporal Action Detection(Localization)问题中最常用的评估指标。一般对IoU=0.5的进行对比，IoU是时间上的交并。
任务特点与难点

主体框架

一般而言，时序行为检测算法可以分为两个阶段：Proposal + Classification。
proposal是指对于一段未分割的长视频，检测出其包含的行为的时序边界，从而产生proposal送入后续的分类阶段。这一阶段算法产生的proposal与真实行为片段(Ground Truth)会产生重叠(IoU, Intersection over Union)，IoU越大，候选的proposal越少，这说明这一阶段产生的时序片段质量越高。
Classsification是指对产生的proposal进行分类，得到其行为动作所对应的类别。

特点

Temporal Action Detection是在Action Recognition问题的基础上发展的。Action Recognition指给定一段已经分割好的视频，算法直接给出其分类结果即可。而Temporal Action Detection则先要对未分割的视频检测，得到其中的一个或多个action instance，然后再分别进行分类识别。这两者的关系与image classfication同object detection的关系十分相似。
目前较为经典的action recognition的方法有iDT, C3D, 2 stream等，故而Temporal Action Detection也可以借鉴了这些方法。

难点

行为的时序边界不清晰。在目标检测中，物体目标的边界通常都是非常明确的，所以可以标注出较为明确的边界框。但时序行为的边界很多时候并不是很明确，什么时候一个行为算开始，什么时候行为算结束常常无法给出一个准确的边界（指精确的第几帧）。
如何较好的结合时序信息。在时序行为检测中，无法只使用静态的图像信息的，必须结合时序的信息，比如使用RNN读入每帧图像上用CNN提取的特征，或是用时序卷积等。故而通常采用spatial和temporal同时进行识别。
行为片段的时间跨度变化可能非常大。比如在ActivityNet中，最短的行为片段大概1s左右，最长的行为片段则超过了200s。巨大的时长跨度，也使得检测时序动作非常难。

关键点

近些年来，在classification阶段，已经实现了较好的结果。但是高质量的proposal一直没有很好地解决，这是由于时序边界的信息较为单一，且不够清晰。如何在保证AP的情况下，尽可能的提高Proposal的质量，是今后的研究重点方向。

数据库

	Decription	组成	

THUMOS 2014	
该数据集包括行为识别和时序行为检测两个任务。
在时序行为检测任务中，只有20类动作的未分割视频是有时序行为片段标注的，包括200个验证集视频（包含3007个行为片段）和213个测试集视频（包含3358个行为片段）。这些经过标注的未分割视频可以被用于训练和测试时序行为检测模型。
地址。
	
训练集：UCF101数据集。包括101类动作，共13320段分割好的视频片段。
验证集：共200个视频有时序行为标注(共3007个行为片 段，只有20类，可用于时序动作检测任务)  (整个验证集包括1010个未分割的视频，包括了分类的任务，对于时序检测无用)
测试集：共213个视频有时序行为标注(共3358个行为片段，只有20类，可用于时序动作检测任务)((整个测试集包括1574个未分割的视频，包括了分类的任务，对于时序检测无用)
270和1496为错误标注
	实际上之后还有THUMOS Challenge 2015,包括更多的动作类别和视频数，但由于上面可以比较的方法不是很多，所以目前看到的文章基本上还是在THUMOS14上进行实验。
MEXaction2	
MEXaction2数据集中包含两类动作：骑马和斗牛。该数据集由三个部分组成：YouTube视频，UCF101中的骑马视频以及INA视频
数据集地址。
	其中YouTube视频片段和UCF101中的骑马视频是分割好的短视频片段，被用于训练集。而INA视频为多段长的未分割的视频，时长共计77小时，且被分为训练，验证和测试集三部分。训练集中共有1336个行为片段，验证集中有310个行为片段，测试集中有329个行为片断。且MEXaction2数据集的特点是其中的未分割视频长度都非常长，被标注的行为片段仅占视频总长的很低比例	

ActivityNet	
目前最大的数据库，同样包含分类和检测两个任务。数据集地址为Activity Net ，这个数据集仅提供视频的youtube链接，而不能直接下载视频，所以还需要用python中的youtube下载工具来自动下载。该数据集包含200个动作类别，20000（训练+验证+测试集）左右的视频，视频时长共计约700小时。
包括两个版本：v1.2和v1.3




	
Activity Net v1.2
 包括100类共9682个视频
训练集:验证集:测试集=2:1:1
Activity Net v1.3
在v1.2数据集上扩充而来。包括200类动作，共19994个视频。现用于ActivityNet Challenge。
平均每段视频发生1.54个行为，共648小时
训练集:验证集:测试集=2:1:1
	


line end